{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO09KccrpJ99zfguQaDLxn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshsingh233/gen_ai/blob/main/gen_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 1\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# write the code to perform basic text processing task the program should create a corpus using user defined words string or text files and generate a vocabulary for the purpose it should then clean the corpus by removing all stop words after cleaning the program must saves te generated vocabs in json format and subsequently load the vocab back from the json file for further use.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "q6EJzxMGn_UB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHaotP6en6Kg"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "SkAwPBcFp4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "1y05SniNqHdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "xKfIUgRkqLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "pjf9meKxqOP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download required nltk resources(run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2VcLXIJqQMs",
        "outputId": "6c1d0e56-7530-4f2a-a35e-6a7f86c1cb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create corpus\n",
        "def create_corpus_from_text(text):\n",
        "    \"\"\"takes user_defined text and returns a list of tokens\"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return tokens\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dhD27mabqoTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus_from_file(file_path):\n",
        "    \"\"\"takes a file path and returns a list of tokens\"\"\"\n",
        "    with open(file_path, 'r',encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Rd5NrQokrTRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean corpus (remove stopwords & punctuations)\n",
        "def clean_corpus(tokens):\n",
        "    \"\"\"takes a list of tokens and returns a list of cleaned tokens\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "_7R-Y3LkttVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate vocabulary\n",
        "def generate_vocabulary(tokens):\n",
        "    \"\"\"create unique vocabulary\"\"\"\n",
        "    vocabulary = sorted(list(set(tokens)))\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "MCesnVx8tuOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save vocabulary to json\n",
        "def save_vocab_to_json(vocab,filename):\n",
        "  \"\"\"save vocabulary to json file\"\"\"\n",
        "  with open(filename,'w') as json_file:\n",
        "    json.dump(vocab,json_file,indent=4)\n",
        "  print(f\"Vocabulary saved to {filename}\")"
      ],
      "metadata": {
        "id": "jWvzv_8ouW-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load vocabulary from json\n",
        "def load_vocab_from_json(filename):\n",
        "  \"\"\"load vocabulary from json file\"\"\"\n",
        "  with open(filename,'r') as json_file:\n",
        "    vocab = json.load(json_file)\n",
        "    print(f\"Vocabulary loaded from {filename}\")\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "eB3lR6AluuJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main execution\n",
        "if __name__ == \"__main__\":\n",
        "  #example\n",
        "  user_text=\"Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that enables computers to understand, interpret, and generate human language in a meaningful way.  It combines computational linguistics, machine learning, and deep learning to process both written and spoken language, allowing machines to perform tasks like translation, sentiment analysis, chatbot interactions, and voice assistants.\"\n"
      ],
      "metadata": {
        "id": "bzIsfk7HvrMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create corpus\n",
        "corpus = create_corpus_from_text(user_text)"
      ],
      "metadata": {
        "id": "Z1MbPCwwwvP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean corpus\n",
        "cleaned_corpus = clean_corpus(corpus)"
      ],
      "metadata": {
        "id": "9zM3tm92w1Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate vocabulary\n",
        "vocab = generate_vocabulary(cleaned_corpus)"
      ],
      "metadata": {
        "id": "wAUe71Xuw4Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save vocabulary\n",
        "save_vocab_to_json(vocab,'vocab.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZj7Lt0txEvq",
        "outputId": "e6ecbb0d-fca3-477f-826c-16b305533362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved to vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load vocabulary\n",
        "loaded_vocab = load_vocab_from_json('vocab.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJn1WEZ2xIRL",
        "outputId": "20b55af8-bef4-4de6-af33-cdaeec1f1990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary loaded from vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display results\n",
        "print(\"Corpus:\",corpus)\n",
        "print(\"Cleaned Corpus:\",cleaned_corpus)\n",
        "print(\"Vocabulary:\",vocab)\n",
        "print(\"Loaded Vocabulary:\",loaded_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAh0DOxIxu-m",
        "outputId": "3c0a581c-8233-498a-cb34-96f33e941ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: ['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'ai', ')', 'that', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', '.', 'it', 'combines', 'computational', 'linguistics', ',', 'machine', 'learning', ',', 'and', 'deep', 'learning', 'to', 'process', 'both', 'written', 'and', 'spoken', 'language', ',', 'allowing', 'machines', 'to', 'perform', 'tasks', 'like', 'translation', ',', 'sentiment', 'analysis', ',', 'chatbot', 'interactions', ',', 'and', 'voice', 'assistants', '.']\n",
            "Cleaned Corpus: ['natural', 'language', 'processing', 'nlp', 'subfield', 'artificial', 'intelligence', 'ai', 'enables', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'meaningful', 'way', 'combines', 'computational', 'linguistics', 'machine', 'learning', 'deep', 'learning', 'process', 'written', 'spoken', 'language', 'allowing', 'machines', 'perform', 'tasks', 'like', 'translation', 'sentiment', 'analysis', 'chatbot', 'interactions', 'voice', 'assistants']\n",
            "Vocabulary: ['ai', 'allowing', 'analysis', 'artificial', 'assistants', 'chatbot', 'combines', 'computational', 'computers', 'deep', 'enables', 'generate', 'human', 'intelligence', 'interactions', 'interpret', 'language', 'learning', 'like', 'linguistics', 'machine', 'machines', 'meaningful', 'natural', 'nlp', 'perform', 'process', 'processing', 'sentiment', 'spoken', 'subfield', 'tasks', 'translation', 'understand', 'voice', 'way', 'written']\n",
            "Loaded Vocabulary: ['ai', 'allowing', 'analysis', 'artificial', 'assistants', 'chatbot', 'combines', 'computational', 'computers', 'deep', 'enables', 'generate', 'human', 'intelligence', 'interactions', 'interpret', 'language', 'learning', 'like', 'linguistics', 'machine', 'machines', 'meaningful', 'natural', 'nlp', 'perform', 'process', 'processing', 'sentiment', 'spoken', 'subfield', 'tasks', 'translation', 'understand', 'voice', 'way', 'written']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "explore text tokennization in nltk by splitting text into sentences words and white space tokens creating custom pattern with regex using the tree band tokenniser for format text and handling hastags mention emogies and url with tweet tokenniser\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fgOqDjRDlhw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "jO6nFVasmcDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7MQfp37mohr",
        "outputId": "b11841f1-7c7d-49be-9fff-2fbcefa14c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize,WhitespaceTokenizer,RegexpTokenizer,TreebankWordTokenizer,TweetTokenizer"
      ],
      "metadata": {
        "id": "w_1DW557mylt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hello ansh this is my good name.let's have some fun using nltk.\"\n",
        "tweet_text=\"loving nlp!!! üòç check https://nltk.org @nltk_org #nproc\""
      ],
      "metadata": {
        "id": "UM49nw7rm7k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWmYwD_gnwMX",
        "outputId": "f88fe6e3-47e6-402a-f939-ca6c65136f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello ansh this is my good name.let's have some fun using nltk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCSxjdFDn6D0",
        "outputId": "2c9cc88e-6aa9-4559-b6ae-f874490964f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"hello ansh this is my good name.let's have some fun using nltk.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0OckPBin9BF",
        "outputId": "b4c508f5-27a1-42dd-80f0-8189dae62bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'ansh', 'this', 'is', 'my', 'good', 'name.let', \"'s\", 'have', 'some', 'fun', 'using', 'nltk', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wt=WhitespaceTokenizer()\n",
        "print(wt.tokenize(text))#white spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4lXoWdLokyF",
        "outputId": "b1be86c7-ac4b-45d1-8fe7-28ead6799457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'ansh', 'this', 'is', 'my', 'good', \"name.let's\", 'have', 'some', 'fun', 'using', 'nltk.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regex_tok=RegexpTokenizer(r'[A-Za-z]+')\n",
        "print(regex_tok.tokenize(text))#only words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuKgLdpnortR",
        "outputId": "1103d6da-5120-496c-9207-9c97e7b49486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'ansh', 'this', 'is', 'my', 'good', 'name', 'let', 's', 'have', 'some', 'fun', 'using', 'nltk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_tok=TreebankWordTokenizer()\n",
        "print(treebank_tok.tokenize(\"they'll save and invest more.\"))#formal text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPsAEVtopB5x",
        "outputId": "9b7a55a3-a905-4719-be71-0868507e0a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['they', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tok=TweetTokenizer()\n",
        "print(tweet_tok.tokenize(tweet_text))#handle emojis hastags mentions urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt-wz-CtpS8H",
        "outputId": "45d69b37-9efd-46aa-bcef-809f93e41cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loving', 'nlp', '!', '!', '!', 'üòç', 'check', 'https://nltk.org', '@nltk_org', '#nproc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tok2=TweetTokenizer(strip_handles=True,reduce_len=True)\n",
        "print(tweet_tok2.tokenize(tweet_text))#remove mention reduce length\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI6hd4FapajT",
        "outputId": "515cf324-1454-4b1c-a139-0bd5af5f3697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loving', 'nlp', '!', '!', '!', 'üòç', 'check', 'https://nltk.org', '#nproc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An online job platform allows users to search for job listings using free-text queries. The platform stores thousands of job descriptions and user queries in english. However the search engine often fails to return relevant results because different grammatical forms of the same word are treated as separate terms. For example, job descriptions may contain words like \"developing\",\"developer\",and \"development\", while users may search for \"develop\". This mismatch reduces the accuracy of search result to improve search relevance and redue vocabulary size the platform plans to apply stemming as part of its text preprocessing pipeline.\n",
        "you are part of a data science learn responsible for enhancing the search functionality of the job portal.Your tast is to analyse how stemming can help normalize textual date and compare the behaviour of different temming techniques. Consider a dataset containing\n",
        "a.job descriptions\n",
        "b.user search queries\n",
        "  "
      ],
      "metadata": {
        "id": "pQPIE7Dvsl8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer"
      ],
      "metadata": {
        "id": "vp35GH4dwElk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\",\"evolving\",\"dissolving\",\"stemming\",\"happily\",\"disadvantage\",\"advantage\"]\n"
      ],
      "metadata": {
        "id": "hHDFCmh8x0hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter=PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "snowball=SnowballStemmer(language='english')"
      ],
      "metadata": {
        "id": "sh8grkdMyTjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word\".ljust(15),\"porter\".ljust(12),\"lancaster\".ljust(12),\"snowball\")\n",
        "print(\"-\"*55)\n",
        "for word in words:\n",
        "    print(word.ljust(15), porter.stem(word).ljust(12), lancaster.stem(word).ljust(12), snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doNK3FUWy1oB",
        "outputId": "96b2817e-d10e-41fa-8d3f-b15ad952ccd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word            porter       lancaster    snowball\n",
            "-------------------------------------------------------\n",
            "running         run          run          run\n",
            "evolving        evolv        evolv        evolv\n",
            "dissolving      dissolv      dissolv      dissolv\n",
            "stemming        stem         stem         stem\n",
            "happily         happili      happy        happili\n",
            "disadvantage    disadvantag  disadv       disadvantag\n",
            "advantage       advantag     adv          advantag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "CGczj2fJzf7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,SnowballStemmer"
      ],
      "metadata": {
        "id": "cgOjSgG30rh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "nMP-wyss01ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afgKiEjp034X",
        "outputId": "c3be0348-d69d-4523-9300-462ba465739f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_descriptions = [\"we are developing scalable software systems\",\n",
        "                    \"Looking for exprienced developers\",\n",
        "                    \"Software development and testing roles avalable\"]"
      ],
      "metadata": {
        "id": "nRZrBCIc09yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_queries=[\"develop\",\"software developer\",\"testing jobs\"]"
      ],
      "metadata": {
        "id": "-nk-vx8c1ZAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_descriptions,user_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2eC2c_1pJt",
        "outputId": "2f803076-05df-44c0-e9d7-efd56f121ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['we are developing scalable software systems',\n",
              "  'Looking for exprienced developers',\n",
              "  'Software development and testing roles avalable'],\n",
              " ['develop', 'software developer', 'testing jobs'])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "EwvbWo7W1u33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()"
      ],
      "metadata": {
        "id": "mehbCWUh2QoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball=SnowballStemmer(language='english')"
      ],
      "metadata": {
        "id": "XcaRbfAL2TVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_tokens(tokens,stemmer):\n",
        "    return [stemmer.stem(t) for t in tokens]"
      ],
      "metadata": {
        "id": "a_fXlXVO2VuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in job_descriptions:\n",
        "    tokens = clean_text(text)\n",
        "    print('original:',tokens)\n",
        "    print('porter:',stem_tokens(tokens,porter))\n",
        "    print('snowball:',stem_tokens(tokens,snowball))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUGF7klX2fqm",
        "outputId": "69ed169b-9828-46ce-e287-82d101c30608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: ['we', 'are', 'developing', 'scalable', 'software', 'systems']\n",
            "porter: ['we', 'are', 'develop', 'scalabl', 'softwar', 'system']\n",
            "snowball: ['we', 'are', 'develop', 'scalabl', 'softwar', 'system']\n",
            "\n",
            "original: ['looking', 'for', 'exprienced', 'developers']\n",
            "porter: ['look', 'for', 'exprienc', 'develop']\n",
            "snowball: ['look', 'for', 'exprienc', 'develop']\n",
            "\n",
            "original: ['software', 'development', 'and', 'testing', 'roles', 'avalable']\n",
            "porter: ['softwar', 'develop', 'and', 'test', 'role', 'aval']\n",
            "snowball: ['softwar', 'develop', 'and', 'test', 'role', 'aval']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"original\".ljust(15),\"porter\".ljust(12),\"snowball\")\n",
        "print(\"-\"*40)\n",
        "for text in job_descriptions:\n",
        "    tokens = clean_text(text)\n",
        "    print(text.ljust(15),stem_tokens(tokens,porter),stem_tokens(tokens,snowball))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UotSpSmZ3YTA",
        "outputId": "bfe9f074-97fc-480b-d010-e2c46be5c28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original        porter       snowball\n",
            "----------------------------------------\n",
            "we are developing scalable software systems ['we', 'are', 'develop', 'scalabl', 'softwar', 'system'] ['we', 'are', 'develop', 'scalabl', 'softwar', 'system']\n",
            "Looking for exprienced developers ['look', 'for', 'exprienc', 'develop'] ['look', 'for', 'exprienc', 'develop']\n",
            "Software development and testing roles avalable ['softwar', 'develop', 'and', 'test', 'role', 'aval'] ['softwar', 'develop', 'and', 'test', 'role', 'aval']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An online customer support system recieve thousands of support tickets everyday . User describe their issue in natural language, using different grammatical forms of words. For example customer may use words running, ran, runs or run . To describe different actions the support system uses text based analylsis to categories tickets identify common issues route tickets to appropriate support teams. However variations in word forms increase the vocabulary in word forms increase the vocab size and reduce the effectiveness of text analysis. To address this problem the system plans to introduce lemmantization as apart of its text preprocessing pipeline .\n",
        "\n",
        "Problem statement: you are a member of data analytics team responsible for improving the text processing module of the customer supprt system your task is to apply lemmantization to normalize the textual data while preserving the original meaning of words the system will use word-net lemmantizer from the nltk library to convert word into their base dictionary form."
      ],
      "metadata": {
        "id": "Cc_7olt4gXpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "import string"
      ],
      "metadata": {
        "id": "awIcFYGD5kOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27sfeGDyk7LT",
        "outputId": "f03f42d2-3e31-4c6c-8c8f-6296bf10d54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tickets=[\n",
        "    \"customers are running into login issues\",\n",
        "    \"user ran into payment failures\",\n",
        "    \"payments are failing and users runs retry\",\n",
        "    \"system was crashing while processing orders\",\n",
        "    \"orders processed successfully after fixes\"\n",
        "]"
      ],
      "metadata": {
        "id": "u_Hu8SISlZqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmantizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "G-qRCugZmGkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(tag):\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN"
      ],
      "metadata": {
        "id": "ChlvDFmimVEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmantize_text(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [t for t in tokens if t not in string.punctuation]\n",
        "  tagged = nltk.pos_tag(tokens)\n",
        "  lemmans = [lemmantizer.lemmatize(w,get_wordnet_pos(p)) for w,p in tagged]\n",
        "  return tokens, lemmans"
      ],
      "metadata": {
        "id": "23KmHnZwmnhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_before = []\n",
        "all_after = []"
      ],
      "metadata": {
        "id": "Ai15WXkSohSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in tickets:\n",
        "  before, after = lemmantize_text(t)\n",
        "  all_before.append(before)\n",
        "  all_after.append(after)\n",
        "  print(\"original:\",before)\n",
        "  print(\"lemmas:\",after)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEi1YgjCord3",
        "outputId": "5c474cdc-bfc1-4fc8-c44c-a439aa593c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: ['customers', 'are', 'running', 'into', 'login', 'issues']\n",
            "lemmas: ['customer', 'be', 'run', 'into', 'login', 'issue']\n",
            "\n",
            "original: ['user', 'ran', 'into', 'payment', 'failures']\n",
            "lemmas: ['user', 'run', 'into', 'payment', 'failure']\n",
            "\n",
            "original: ['payments', 'are', 'failing', 'and', 'users', 'runs', 'retry']\n",
            "lemmas: ['payment', 'be', 'fail', 'and', 'user', 'run', 'retry']\n",
            "\n",
            "original: ['system', 'was', 'crashing', 'while', 'processing', 'orders']\n",
            "lemmas: ['system', 'be', 'crash', 'while', 'process', 'order']\n",
            "\n",
            "original: ['orders', 'processed', 'successfully', 'after', 'fixes']\n",
            "lemmas: ['order', 'process', 'successfully', 'after', 'fix']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"unique words before lemmantization\",len(set(sum(all_before,[]))))\n",
        "print(\"unique words after lemmantization\",len(set(sum(all_after,[]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIp_rjoppBT5",
        "outputId": "49c1ee1c-c596-460b-9596-babcfb28e1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words before lemmantization 26\n",
            "unique words after lemmantization 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An online movie review platform collects user review to analyse and highlight common issues. Reviews often contain a large number of common words such as the, is, a, and, of, which do not carry significant meaning. These common words stop words make the vocabulary unnecessary large and can reduce the performance of text analysis models.\n",
        "\n",
        "Problem statement: You are a part of nlp team  taked with preprocessing textual reviews to improve sentiment analysis and keyword extraction .Your task is to identify and remove stop words from the text data using nltk stop word list. You will also analyse how stop word removal affects the vocabulary size and the meaningful content retained."
      ],
      "metadata": {
        "id": "sjogxXOXsCjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "ebg8IDqEqa1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ukdAgK4umIP",
        "outputId": "98defc6b-520c-47e2-9704-09d22516c209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews=[\n",
        "    \"The movie was absolutely amazing and the acting was great\",\n",
        "    \"I did not like the film, it was boring and too long\",\n",
        "    \"The story is good but the direction is weak\",\n",
        "    \"An excellent movie with s powerful message\"\n",
        "]"
      ],
      "metadata": {
        "id": "K4ss2A9kuzqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(reviews).lower()\n"
      ],
      "metadata": {
        "id": "qfZPZ_Ruvklk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "QpnKIBOMvq-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before = set([w for w in tokens if w.isalpha()])"
      ],
      "metadata": {
        "id": "CpfMjMFrvuzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "MqT-0O-ZwAaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]"
      ],
      "metadata": {
        "id": "8LPwgF9NwXYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_after = set(filtered_tokens)"
      ],
      "metadata": {
        "id": "-Z_fZWrdwlrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary size BEFORE:\",len(vocab_before))\n",
        "print(\"Vocabulary size AFTER:\",len(vocab_after))\n",
        "print(\"\\nTop meaning words:\",Counter(filtered_tokens).most_common(10))\n",
        "print(\"\\nCleaned tokens:\",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPQuGYodw00F",
        "outputId": "b1e1943c-5718-415a-d5a6-78dc57db81c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size BEFORE: 29\n",
            "Vocabulary size AFTER: 16\n",
            "\n",
            "Top meaning words: [('movie', 2), ('absolutely', 1), ('amazing', 1), ('acting', 1), ('great', 1), ('like', 1), ('film', 1), ('boring', 1), ('long', 1), ('story', 1)]\n",
            "\n",
            "Cleaned tokens: ['movie', 'absolutely', 'amazing', 'acting', 'great', 'like', 'film', 'boring', 'long', 'story', 'good', 'direction', 'weak', 'excellent', 'movie', 'powerful', 'message']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An online job portal receives resumes and job descriptions in free-text format. To extract key information such as skills, responsible, and actions, the system needs to understand the grammetical structure of sentences. Parts of speech(POS) tagging helps identify whether a word is a noun, verb, adjective, etc. This downstream modules to focus on meaningful words while ignoring less informative ones like determiners, conjunctions, or prepositions. By filtering only nouns, verbs, adjectives, the system can:\n",
        "extract skills , technologies and tools (nouns)\n",
        "capture actions and responsibilities (verbs)\n",
        "highlight descriptive qualities (adjectives)\n",
        "This step reduces noise and improves the effectiveness of information extraction, keyword identification, and resume-job matching.\n",
        "\n",
        "Problem Statement: You are part of the NLP team tasked with preprocessing textual resumes and job descriptions. Your tasks are: preprocess text, tokenize sentences and words, remove punctuation, apply POS tagging to all words, analyze and extract meaningful words (nouns, verbs, adjectives) from the POS-tagged text. Display: Original tokens and POS tags, list of meaningful words.\n",
        "\n"
      ],
      "metadata": {
        "id": "ch_H1XdLyWcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "text = \"\"\"Experienced software engineer skilled in Python, C++, and machine learning.\n",
        "Developed scalable applications and optimized algorithms.\n",
        "Strong analytical and problem-solving abilities.\"\"\"\n",
        "\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "\n",
        "\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "\n",
        "meaningful_words = [word for word, tag in pos_tags if tag.startswith('N') or tag.startswith('V') or tag.startswith('J')]\n",
        "\n",
        "\n",
        "print(\"Original Tokens with POS Tags:\")\n",
        "print(pos_tags)\n",
        "print(\"\\nMeaningful Words:\")\n",
        "print(meaningful_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxroHhL5j7FZ",
        "outputId": "2e34da8c-0383-4199-d88f-b66566981c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens with POS Tags:\n",
            "[('experienced', 'JJ'), ('software', 'NN'), ('engineer', 'NN'), ('skilled', 'VBD'), ('in', 'IN'), ('python', 'JJ'), ('c', 'NN'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('developed', 'VBD'), ('scalable', 'JJ'), ('applications', 'NNS'), ('and', 'CC'), ('optimized', 'VBN'), ('algorithms', 'NN'), ('strong', 'JJ'), ('analytical', 'JJ'), ('and', 'CC'), ('problemsolving', 'VBG'), ('abilities', 'NNS')]\n",
            "\n",
            "Meaningful Words:\n",
            "['experienced', 'software', 'engineer', 'skilled', 'python', 'c', 'machine', 'learning', 'developed', 'scalable', 'applications', 'optimized', 'algorithms', 'strong', 'analytical', 'problemsolving', 'abilities']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In modern nlp application organisation often need to extract structured information from large amount of unstructured text then identify who what where when in text is crucial in search analysis. So NER(Named Entity Recognition) is a key preprocessing technique that detects and classify proper noun into predefined categories like person organisation and location name date time and money. For example, in resumes, NER can identify candidate names, companies, locations and important dates news article it can detect organisation event and places by converting unstructured text into structured entities. NER enables downstream nlp models to extract meaning ful information.\n",
        "problem statement:Your objective is to apply name recognition on a sample text using spacy and identify entities such as person names organisaions, locations, dates and money. After extraction you should display each entity along with its catrgory, count the frequency of each entity type and optionally extract entities by type for further analysis. The goal is to understand how NER converts unstructured text into meaning ful data and how this structured information can support donstream task such as parsing , automated reporting , keyword extraction or ques/ans.\n",
        "\n"
      ],
      "metadata": {
        "id": "sYc8xgHOt3WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import subprocess,sys"
      ],
      "metadata": {
        "id": "G20DGPyfzKgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "  subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "zY08KOcu5e_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Rohit Sharma has joined INFOSYS in Banglore in June 2021 with a salary 5000 dollars per month.\n",
        "He previously worked at TCS from 2018 to 2020\"\"\""
      ],
      "metadata": {
        "id": "UVg3DBhK5yuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "WxhAh3_e6gxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = [(ent.text,ent.label_) for ent in doc.ents]"
      ],
      "metadata": {
        "id": "kGDz2Aws6kgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nExtracted entities\")\n",
        "for ent,label in entities:\n",
        "  print(f\"{ent} ({label})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KGnjaoQ6wJ9",
        "outputId": "76ee5e25-835c-4692-aa41-6dbc88d59887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted entities\n",
            "Rohit Sharma (PERSON)\n",
            "INFOSYS (ORG)\n",
            "Banglore (GPE)\n",
            "June 2021 (DATE)\n",
            "5000 dollars (MONEY)\n",
            "TCS (ORG)\n",
            "2018 (DATE)\n",
            "2020 (DATE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = Counter([ent.label_ for ent in doc.ents])"
      ],
      "metadata": {
        "id": "cR5WRsz56_sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('item entity type frequency')\n",
        "for label,count in label_counts.items():\n",
        "  print(f\"{count} {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snH37Hff7V6u",
        "outputId": "ca9590d3-18a7-4dce-8877-fd5589d754d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item entity type frequency\n",
            "1 PERSON\n",
            "2 ORG\n",
            "1 GPE\n",
            "3 DATE\n",
            "1 MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_dict = {}\n",
        "for ent in doc.ents:\n",
        "  entity_dict.setdefault(ent.label_,[]).append(ent.text)"
      ],
      "metadata": {
        "id": "8mqRDn1_7pbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n Entities group by types')\n",
        "for label,ents in entity_dict.items():\n",
        "  print(f\"{label} {ents}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M2vMPvg750M",
        "outputId": "46e04174-8215-4ff3-f583-e5f707b4b1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Entities group by types\n",
            "PERSON ['Rohit Sharma']\n",
            "ORG ['INFOSYS', 'TCS']\n",
            "GPE ['Banglore']\n",
            "DATE ['June 2021', '2018', '2020']\n",
            "MONEY ['5000 dollars']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorisation - after creating the vocab we create the number on the basis of word present in the sentence.\n",
        "\n",
        "one hot encoding = big matrix size\n",
        "bow technique = frequency array\n"
      ],
      "metadata": {
        "id": "Z68ALjUFofnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset\n",
        "data preprocessing\n",
        "vectorisation\n",
        "model implementation\n",
        "evaluate\n"
      ],
      "metadata": {
        "id": "IGLPkt0vq_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "sZ3Ia6Qt8Mwl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tH3zMf2_sHEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}